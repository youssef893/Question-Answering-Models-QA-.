# Question-Answering-Models-QA-.

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpencerPao/Natural-Language-Processing/blob/main/Question_Answering_Modeling_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1961132",
      "metadata": {
        "id": "e1961132"
      },
      "source": [
        "# Question Answering Models (QA)\n",
        "\n",
        "## A Deep Learning Model that can answer all your questions.... if good of course.\n",
        "\n",
        "### There are 2 different categories of QA Modeling\n",
        "- Domain - systems that are constrained by the input data; we have open and closed systems\n",
        "    - <b> Open domain systems </b> are for broad questions, not specific to what category of discussion (Wikipedia, World wide web, Alexa, etc...)\n",
        "    - <b> Closed domain systems </b> are more narrow in its vocabulary and focus on a specific industry or topic (Football, Finance, Tech, Law etc..) \n",
        "- Question Type - Open ended questions, Yes/No questions, inference questions etc...)\n",
        "    - Once you determine what type of system you want to establish, you then need to figure out which question type you want your model to focus on\n",
        "\n",
        "\n",
        "\n",
        "### Types of Question Answering Models\n",
        "- <i> Extractive Question Answering </i> is a deep learning model that can provide an answer when given a corpus of text (i.e context). So, when you provide a question to the model, the model then \"searches\" the documents to pinpoint the best answer to the question. It's essentially a searching tool in many ways... \n",
        "- <i> Open Generative Question Answering </i> is a deep learning model that generates text based on context. So, unlike the extractive question answering model, the answer does not <b> literally </b> have to be in the text.\n",
        "- <i> Closed Generative Question Answering </i> is a deep learning model where no context is provided and the answer is generated by the model. \n",
        "\n",
        "More information about the 3 subsets of Question Answering Modelings can be found [at HuggingFace.co](https://huggingface.co/tasks/question-answering) -- we'll be <b> focusing more on the <i> Extractive Question Answering </i> model in this notebook. </b>\n",
        "\n",
        "- The QA datasets are laboriously tailored and tagged to fit the \"mold\" of a QA model. A training dataset will look something like this: \n",
        "    - _Also note that QA training data does not necessarily need an associated answer._\n",
        "\n",
        "\n",
        "### Resources\n",
        "- NLP-Progress of [current state of the art NLP tasks](http://nlpprogress.com/english/question_answering.html) --> Contains QA datasets commonly used for state of the art QA NLP models. \n",
        "\n",
        "- [HuggingFace repositories](https://huggingface.co/); This houses many popular NLP QA Models, some of which I will use in this notebook.\n",
        "\n",
        "- [QA metadata format explanation](https://simpletransformers.ai/docs/qa-data-formats/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96825443",
      "metadata": {
        "id": "96825443"
      },
      "source": [
        "# Extractive QA Model Structure and Use\n",
        "\n",
        "[Check out the BERT Video I did here!](https://www.youtube.com/watch?v=72Ylk77PqR8)\n",
        "\n",
        "- The QA Models are essentially extensions of the BERT model with slightly different ouput layers\n",
        "- Interested in the base mode [BERT?](https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/); Here is the actual [paper of RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n",
        "- Link to [documentation for RobERTa](https://www.geeksforgeeks.org/overview-of-roberta-model/)\n",
        "    - RoBERTa is a new and improved version of BERT\n",
        "        - Removes the Next Sentence Prediction (NSP) objective\n",
        "        - Trained on bigger batch sizes & longer sequences\n",
        "        - Dynamically changes the masking pattern\n",
        "        - TRAINED on a large corpus of English data with <b> no </b> labeling whatsoever. (just the raw texts)\n",
        "            - Masks 15% of the input; RoBERTa runs the entire masked sentence through the model and the model attempts to predict the masked terms correctly.\n",
        "            - This is where the QA model learns the context and have a basic understanding to the language modeling!\n",
        "\n",
        "Using the very popular question-answering model: **RoBERTa-base for QA** \n",
        "\n",
        "    \"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\" - deepset.ai"
      ]
    }
